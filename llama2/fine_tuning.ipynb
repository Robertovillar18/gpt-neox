{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('huggingface-cli login --token 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')\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, logging\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from peft import AutoPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from random import randrange\n",
    "from functools import partial\n",
    "#from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and tokenizer names\n",
    "base_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "refined_model = \"llama-2-7b-enhanced\"\n",
    "\n",
    "def load_model(model_name_):\n",
    "  device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
    "  print('La ejecución se realizará en:',device)\n",
    "\n",
    "  # Tokenizer\n",
    "  llama_tokenizer = AutoTokenizer.from_pretrained(model_name_, trust_remote_code=True)\n",
    "  llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "  llama_tokenizer.padding_side = \"right\"  # Fix for fp16\n",
    "  llama_tokenizer.device_map = device\n",
    "\n",
    "  # Quantization Config\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "      load_in_4bit=True,\n",
    "      bnb_4bit_quant_type=\"nf4\",\n",
    "      bnb_4bit_compute_dtype=torch.float16,\n",
    "      bnb_4bit_use_double_quant=False\n",
    "  )\n",
    "\n",
    "  # Model\n",
    "  base_model = AutoModelForCausalLM.from_pretrained(\n",
    "      model_name_,\n",
    "      quantization_config=quant_config,\n",
    "      device_map=device\n",
    "  )\n",
    "\n",
    "  return base_model, llama_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model, llama_tokenizer = load_model(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(query_, model_, tokenizer_): \n",
    "  device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
    "  print('La ejecución se realizará en:', device)\n",
    "\n",
    "  model = model_\n",
    "\n",
    "  sys = '<<SYS>>Eres un asistente de tramites del estado uruguayo. Das respuestas lo mas resumidas posible.'+\\\n",
    "  'Eres un asistente que responde en español.'+\\\n",
    "  'Eres un asistente servicial, respetuoso y honesto. Responde siempre de forma resumida de la manera más útil posible y siendo seguro.'+\\\n",
    "  'Tus respuestas no deben incluir ningún contenido dañino, poco ético, racista, sexista, tóxico, peligroso o ilegal.'+\\\n",
    "  'Asegúrese de que sus respuestas sean socialmente imparciales y de naturaleza positiva.'+\\\n",
    "  'Si una pregunta no tiene ningún sentido o no es objetivamente coherente, explique por qué en lugar de responder algo que no sea correcto.'+\\\n",
    "  'Si no sabe la respuesta a una pregunta, no comparta información falsa.'+\\\n",
    "  'Solo respondes en español con información relacionada al pais Uruguay.'\n",
    "\n",
    "  query = query_\n",
    "  inputs = tokenizer_(f'<s>[INST] {sys}\" input:\" {query} [/INST] response:', return_tensors=\"pt\").to(device)\n",
    "  outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"),\n",
    "                            attention_mask=inputs[\"attention_mask\"],\n",
    "                            max_new_tokens=500,\n",
    "                            pad_token_id=tokenizer_.eos_token_id,\n",
    "                            temperature=0.1,\n",
    "                            do_sample=True,\n",
    "                            top_k=20,\n",
    "                            num_return_sequences=1,\n",
    "                            repetition_penalty=1.1\n",
    "                          )\n",
    "  response = tokenizer_.decode(outputs[0], skip_special_tokens=True)\n",
    "  return str(response.split('response:')[-1])\n",
    "\n",
    "qry = \"Donde me puedo sacar la cedula online en uruguay?\"\n",
    "print('query test:', qry)\n",
    "print('-------------------------------------------------------------')\n",
    "print(test_model(qry, base_model, llama_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "data = load_dataset(data_name)['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'tramite_cedula.json'\n",
    "f = open(dataset_name,  encoding='UTF-8')\n",
    "json_data = json.load(f)\n",
    "df_train = pd.DataFrame(json_data['train'])\n",
    "\n",
    "dataset = Dataset.from_pandas(df_train)\n",
    "\n",
    "print(f'Number of prompts: {len(dataset)}')\n",
    "print(f'Column names are: {dataset.column_names}')\n",
    "dataset[randrange(len(dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "\n",
    "  # Initialize static strings for the prompt template\n",
    "  S_KEY_INIT = '<s>'\n",
    "  INSTRUCTION_KEY_INIT = '[INST] '\n",
    "  INSTRUCTION_KEY_END = ' [/INST] '\n",
    "  S_KEY_END = '</s>'\n",
    "\n",
    "  input_text = f'{S_KEY_INIT}{INSTRUCTION_KEY_INIT}'\n",
    "  input_text += f'{sample[\"instruction\"]}{INSTRUCTION_KEY_END}'\n",
    "  lst_data = [sample[\"output\"]]\n",
    "\n",
    "  text = \"\\n\".join(lst_data)\n",
    "  # print(response)\n",
    "  formatted_prompt = f'{input_text}{text} {S_KEY_END}'\n",
    "\n",
    "  global data\n",
    "  data = data.add_item({'text':formatted_prompt})\n",
    "\n",
    "  # Store the formatted prompt template in a new key “text”\n",
    "  sample['text'] = formatted_prompt\n",
    "\n",
    "  return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.map(create_prompt_formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(model):\n",
    "  # Pull model configuration\n",
    "  conf = model.config\n",
    "  # Initialize a “max_length” variable to store maximum sequence length as null\n",
    "  max_length = None\n",
    "  # Find maximum sequence length in the model configuration and save it in 'max_length' if found\n",
    "  for length_setting in ['n_positions', 'max_position_embeddings', 'seq_length']:\n",
    "    max_length = getattr(model.config, length_setting, None)\n",
    "    if max_length:\n",
    "      print(f'Found max lenth: {max_length}')\n",
    "      break\n",
    "  # Set “max_length” to 1024 (default value) if maximum sequence length is not found in the model configuration\n",
    "  if not max_length:\n",
    "    max_length = 1024\n",
    "    print(f'Using default max length: {max_length}')\n",
    "  return max_length\n",
    "\n",
    "get_max_length(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question and Answer Task\n",
    "def compute_metrics(pred):\n",
    "    squad_labels = pred.label_ids\n",
    "    squad_preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    # Calculate Exact Match (EM)\n",
    "    em = sum([1 if any(p == l) else 0 for p, l in zip(squad_preds, squad_labels)]) / len(squad_labels)\n",
    "\n",
    "    # Calculate F1-score\n",
    "    f1 = f1_score(squad_labels[0], squad_preds[0], average='macro')\n",
    "\n",
    "    return {\n",
    "        'exact_match': em,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splited_data = data.train_test_split(test_size=0.2)\n",
    "training_data = splited_data['train']\n",
    "evaluation_data = splited_data['test']\n",
    "print('Total:', len(data),'Train:',len(training_data), 'Eval:',len(evaluation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Config\n",
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "# Training Params\n",
    "train_params = TrainingArguments(\n",
    "    output_dir=\"./results_modified\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=25,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\"\n",
    ")\n",
    "# Trainer\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=training_data,\n",
    "    peft_config=peft_parameters,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=llama_tokenizer,\n",
    "    args=train_params,\n",
    "    compute_metrics=compute_metrics,\n",
    "    eval_dataset=evaluation_data,\n",
    "    use_reentrant=True\n",
    ")\n",
    "# Training\n",
    "fine_tuning.train()\n",
    "# Save Model\n",
    "fine_tuning.model.save_pretrained(refined_model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
