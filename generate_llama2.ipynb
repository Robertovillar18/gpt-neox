{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, getopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "from torch import cuda, bfloat16\n",
    "from transformers import AutoTokenizer, AutoConfig, LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers StoppingCriteria, StoppingCriteriaList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "# device = 'auto'\n",
    "print('La ejecución se realizará en:',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list = ['\\ninput:', '\\n```\\n']\n",
    "\n",
    "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
    "stop_token_ids\n",
    "\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "stop_token_ids\n",
    "\n",
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    # stopping_criteria=stopping_criteria,\n",
    "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    repetition_penalty=1.1,  # without this output begins repeating\n",
    "    # return_full_text=True,  # langchain expects the full text\n",
    "    device_map=device,\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "default_prompt = '<s>[INST]'+\\\n",
    "'<<SYS>>'+\\\n",
    "'Eres un asistente de tramites del estado uruguayo. Das respuestas lo mas resumidas posible.'+\\\n",
    "'Eres un asistente que responde en español.'+\\\n",
    "'Eres un asistente servicial, respetuoso y honesto. Responde siempre de forma resumida de la manera más útil posible y siendo seguro.'+\\\n",
    "'Tus respuestas no deben incluir ningún contenido dañino, poco ético, racista, sexista, tóxico, peligroso o ilegal.'+\\\n",
    "'Asegúrese de que sus respuestas sean socialmente imparciales y de naturaleza positiva.'+\\\n",
    "'Si una pregunta no tiene ningún sentido o no es objetivamente coherente, explique por qué en lugar de responder algo que no sea correcto.'+\\\n",
    "'Si no sabe la respuesta a una pregunta, no comparta información falsa.'+\\\n",
    "'Solo hablas y escribes en español.'+\\\n",
    "'Dirección de trámites del estado de uruguay: https://www.gub.uy/tramites/.'+\\\n",
    "'<</SYS>>\\n'\n",
    "\n",
    "user_input = '\\ninput:{input}'\n",
    "last_part_promp = '</s>'\n",
    "\n",
    "conversation = ''\n",
    "def dialog(text):\n",
    "\n",
    "    global conversation\n",
    "    global default_prompt\n",
    "\n",
    "    user_text = str(user_input).replace('{input}', text)\n",
    "    conversation += user_text\n",
    "\n",
    "    prompt = default_prompt + conversation + '[/INST] \\nresponse: </s>'\n",
    "\n",
    "    sequences = pipeline(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=1024\n",
    "    )\n",
    "    get_clean_answer = ''\n",
    "    for seq in sequences:\n",
    "        row_answer = str(seq['generated_text'])\n",
    "        get_clean_answer =  row_answer.split('[/INST]')[-1].strip()\n",
    "        get_clean_answer =  get_clean_answer.split('</s>')[-1].strip()\n",
    "        conversation += '\\nresponse:' + get_clean_answer\n",
    "        return get_clean_answer\n",
    "    return get_clean_answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv, device):\n",
    "    input = ''\n",
    "    output = ''\n",
    "\n",
    "    try:\n",
    "        opts, args = getopt.getopt(argv,\"hi:o:\",[\"input=\",\"output=\"])\n",
    "    except getopt.GetoptError:\n",
    "        print ('generate_llama2.py -d')\n",
    "        sys.exit(2)\n",
    "\n",
    "    for opt, arg in opts:\n",
    "        if opt in (\"-i\", \"--input\"):\n",
    "            input = arg\n",
    "        elif opt in (\"-o\", \"--output\"):\n",
    "            output = arg\n",
    "        elif opt in (\"-d\"):\n",
    "            text = ''\n",
    "            while text != 'chau':\n",
    "                text = input()\n",
    "                result = dialog(text)\n",
    "                print(result)\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "   main(sys.argv[1:], device)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
